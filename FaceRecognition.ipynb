{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FaceRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agoy2zdvsStE",
        "colab_type": "code",
        "outputId": "2be07d8d-cec9-4423-b995-6ff3d295ce59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "cd drive/My Drive/900Faces"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/900Faces\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt8t9LYGvHwT",
        "colab_type": "code",
        "outputId": "1065d98e-38a1-49ee-8e1f-8696dca8b437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset.7z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y29hnhJZsfq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!7z x dataset.7z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvV5613kRCcV",
        "colab_type": "text"
      },
      "source": [
        "**SOME PROBLEMS WITH THE DATASET**\n",
        "\n",
        "A few problems discovered in the dataset during testing:\n",
        "1. 03MUCT_i429 and 03MUCT_i088 are the same person \n",
        "2. 05faces94_gotone is not a person, but a collection of other people in this dataset (05faces94_astefa, 05faces94_sbains and 10 others)\n",
        "\n",
        "Delete 05faces94_gotone and 03MUCT_i0429 folders. \n",
        "\n",
        "SVC still classifies with an accuracy of over 99% using just two samples per person, even with these mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR-ub0VM_QtT",
        "colab_type": "text"
      },
      "source": [
        "**PREPROCESSING THE DATASET**\n",
        "\n",
        "This section takes a bulk dataset and divides it into a trainset and a testset based on the ratio of photos specified to use for test. If the dataset has been partitioned already, can proceed to train directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6vZrIlMxITx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, random\n",
        "from tqdm import tqdm\n",
        "from shutil import copyfile\n",
        "\n",
        "def create_set_dictionary(directory):\n",
        "\t'''\n",
        "\tReturns a dictionary\n",
        "\n",
        "\tKeys are the names of people\n",
        "\tcorresponding Values are the lists of relative paths to the images\n",
        "\t'''\n",
        "\tdata = {}\n",
        "\n",
        "\tfor folder in tqdm(os.listdir(directory)):\n",
        "\t\tlabel_name = folder\n",
        "\t\tphotos_full_paths = []\n",
        "\t\n",
        "\t\tfor photo in os.listdir(os.path.join(directory, folder)):\n",
        "\t\t\tphotos_full_paths.append(os.path.join(directory, label_name, photo))\n",
        "\t\n",
        "\t\tdata[label_name] = photos_full_paths\n",
        "\treturn data\n",
        "\n",
        "def makdir(path):\n",
        "\tif not os.path.isdir(path):\n",
        "\t\tos.makedirs(path)\n",
        "\n",
        "def split_dataset(folder, label, testratio):\n",
        "\t'''\n",
        "\tCreates trainset and testset directories by randomly choosing the test set\n",
        "\tand using the rest for the train set\n",
        "\n",
        "\tFunction supposed to be run only once for the same dataset\n",
        "\t'''\n",
        "\tall_images = os.listdir(os.path.join(folder, label))\n",
        "\n",
        "\ttest_files = random.sample(all_images, int(testratio*len(all_images)))\n",
        "\n",
        "\ttrain_files = [file for file in all_images if file not in test_files]\n",
        "\n",
        "\tmakdir(os.path.join(test_dir, label))\n",
        "\tmakdir(os.path.join(train_dir, label))\n",
        "\n",
        "\tfor image in test_files:\n",
        "\t\tcopyfile(os.path.join(folder, label, image), os.path.join(test_dir, label, image))\n",
        "  \n",
        "\tfor image in train_files:\n",
        "\t\tcopyfile(os.path.join(folder, label, image), os.path.join(train_dir, label, image))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHY-NeDXxoAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder = \"dataset\" # path to the bulk dataset\n",
        "testratio = 0.35 # 35% of photos for each label will be used for test purpose i.e. 65% for training\n",
        "everything = create_set_dictionary(folder)\n",
        "print(\"created a dictionary of paths inside the dataset\")\n",
        "\n",
        "train_dir = \"trainset\" # train_dir and test_dir variables are required for split_dataset function\n",
        "test_dir = \"testset\"\n",
        "\n",
        "makdir(train_dir)\n",
        "makdir(test_dir)\n",
        "makdir(\"records\") # directory to store record files CSV\n",
        "\n",
        "print(\"partitioning the dataset into train and test\")\n",
        "for label in tqdm(everything):\n",
        "    split_dataset(folder, label, testratio)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a1MU31r2fEU",
        "colab_type": "text"
      },
      "source": [
        "**FACE RECOGNITION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjv9Mutm3y4v",
        "colab_type": "code",
        "outputId": "c492f51f-c2f1-4301-aa55-144a386942a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "! pip install face_recognition"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
            "\u001b[K     |████████████████████████████████| 100.2MB 103kB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_recognition) (1.18.2)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (19.18.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.0.0)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566172 sha256=d37899779cf173e72be6a7068b865c4c3bb33e06bb0dcf2321667d2b22391158\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc1Ci3L32iE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv, json, cv2, os, face_recognition\n",
        "from time import time, ctime, sleep\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "'''\n",
        "Expects directory structure as follows:\n",
        "\n",
        "trainset\n",
        "  -->label1\n",
        "    -->photo1.jpg\n",
        "    -->photo2.jpg\n",
        "  -->label2\n",
        "    -->photo1.jpg\n",
        "    etc\n",
        "\n",
        "testset\n",
        "  -->label1\n",
        "    -->photo1.jpg\n",
        "    -->photo2.jpg\n",
        "  -->label2\n",
        "    -->photo1.jpg\n",
        "  etc\n",
        "'''\n",
        "\n",
        "class Pipeline:\n",
        "    ALGORITHM = None\n",
        "    def __init__(self, train_res, infer_res, train_dir, test_dir):\n",
        "        self.RESIZE_DIM_TRAINING = train_res\n",
        "        self.RESIZE_DIM_INFERENCE = infer_res\n",
        "\n",
        "        self.train_dir = train_dir\n",
        "        self.test_dir = test_dir\n",
        "        self.train_dict = self.create_set_dictionary(self.train_dir)\n",
        "        self.test_dict = self.create_set_dictionary(self.test_dir)\n",
        "\n",
        "    def create_set_dictionary(self, directory):\n",
        "        '''\n",
        "        Returns a dictionary\n",
        "\n",
        "        Keys are the names of people\n",
        "        corresponding Values are the lists of relative paths to the images\n",
        "        '''\n",
        "        data = {}\n",
        "\n",
        "        for folder in os.listdir(directory):\n",
        "            label_name = folder\n",
        "            photos_full_paths = []\n",
        "\n",
        "            for photo in os.listdir(os.path.join(directory, folder)):\n",
        "                photos_full_paths.append(os.path.join(directory, label_name, photo))\n",
        "\n",
        "            data[label_name] = photos_full_paths\n",
        "\n",
        "        return data\n",
        "\n",
        "    def img_resize(self, img_array, resizeto):\n",
        "        '''\n",
        "        Resizes images preserving the aspect ration\n",
        "\n",
        "        input: image's array and a single dimension for the resizing\n",
        "        '''\n",
        "\n",
        "        height, width = img_array.shape[:2]\n",
        "        resize_h, resize_w = 0, 0\n",
        "\n",
        "        if height > width:\n",
        "            if height > resizeto:\n",
        "                resize_h = resizeto\n",
        "                resize_w = int(resize_h * width/height)\n",
        "            else: return img_array\n",
        "        else:\n",
        "            if width >= resizeto:\n",
        "                resize_w = resizeto\n",
        "                resize_h = int(resize_w * height/width)\n",
        "            else: return img_array\n",
        "\n",
        "        new = cv2.resize(img_array, (resize_w, resize_h))\n",
        "\n",
        "        # # to view the photos\n",
        "        # plt.imshow(new, interpolation='nearest')\n",
        "        # plt.show()\n",
        "\n",
        "        return new\n",
        "\n",
        "    def extract_encodings(self, data, no_of_samples, resize=True):\n",
        "        '''\n",
        "        Extracts the encodings of images fed through the dictionary prepared \n",
        "        using `create_set_dictionary`\n",
        "\n",
        "        input: dictionary, no of samples to encode per label\n",
        "\n",
        "        Returns encodings of images and labels which are in order\n",
        "        '''\n",
        "        face_encodings = []\n",
        "        names = []\n",
        "\n",
        "        for person in tqdm(data):\n",
        "            train_samples = 0 # number of training samples whose face encodings have been extracted, just a counter\n",
        "\n",
        "            for image in data[person]:\n",
        "                # print(f\"processing {image}\")\n",
        "                face = face_recognition.load_image_file(image)\n",
        "                if resize: face = self.img_resize(face, self.RESIZE_DIM_TRAINING)\n",
        "\n",
        "                face_bounding_boxes = face_recognition.face_locations(face)\n",
        "\n",
        "                if len(face_bounding_boxes) == 1:\n",
        "                    face_enc = face_recognition.face_encodings(face)[0]\n",
        "                    face_encodings.append(face_enc)\n",
        "                    names.append(person)\n",
        "                    train_samples += 1\n",
        "\n",
        "                    if train_samples >= no_of_samples: break # Make sure we don't take more than {training_samples} no of samples\n",
        "                else:\n",
        "                    print(image + \" can't be used for training and has been skipped\") # No face in the photo, or more than one\n",
        "\n",
        "        return face_encodings, names\n",
        "    \n",
        "    def pipeline(self, low=3, high=4, resize=True, verbose=True): \n",
        "        # using (low=3, high=4) implies 3 samples per label by default\n",
        "        # (low=2, high=5) implies go for three iterations with 2 samples, 3 samples, and 4 samples\n",
        "        for i in range(low, high):\n",
        "            print(f\"USING {i} SAMPLE(s) PER LABEL\")\n",
        "\n",
        "            start = time()\n",
        "   \n",
        "            print(\"training in progress\")\n",
        "            sleep(0.2)\n",
        "            self.train(i, resize=resize)\n",
        "\n",
        "            train_time = time()-start\n",
        "            print(f\"time to encode + train: {train_time}\")\n",
        "\n",
        "            print(\"testing in progress\")\n",
        "            sleep(0.2)\n",
        "            acc = self.test(resize=resize, verbose=verbose)\n",
        "\n",
        "            time_taken = time()-start\n",
        "            print(f\"total time taken: {time_taken}\")\n",
        "\n",
        "            # Add to the records file\n",
        "            self.keep(i, self.algo_spec, acc, train_time, time_taken)\n",
        "\n",
        "    def keep(self, no_of_samples, algo_spec, accuracy, train_time, total_time):\n",
        "        file = ctime()[:10]\n",
        "\n",
        "        file_path = f\"records/{file}.csv\"\n",
        "\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(f\"records/{file}.csv\", \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "                writer = csv.writer(f, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "                writer.writerow([self.ALGORITHM, no_of_samples, self.RESIZE_DIM_TRAINING, self.RESIZE_DIM_INFERENCE, \\\n",
        "                    json.dumps(algo_spec,  ensure_ascii=False), accuracy, train_time, total_time])\n",
        "        else:\n",
        "            with open(f\"records/{file}.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "                writer = csv.writer(f, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "                writer.writerow([\"algorithm\", \"no of samples\", \"train_resize_dim\", \"test_resize_dim\", \"algorithm specific\", \"accuracy\", \"train time\", \"total time\"])\n",
        "            self.keep(no_of_samples, algo_spec, accuracy, train_time, total_time)\n",
        "\n",
        "class byDistance(Pipeline):\n",
        "    def __init__(self, train_res, infer_res, train_dir, test_dir, distance_cutoff):\n",
        "        super().__init__(train_res, infer_res, train_dir, test_dir)\n",
        "        self.ALGORITHM = \"distance\"\n",
        "        self.cutoff = distance_cutoff\n",
        "        self.algo_spec = {\"cutoff\": distance_cutoff}\n",
        "\n",
        "    def train(self, no_of_samples, resize=True):\n",
        "        self.encodings, self.labels = self.extract_encodings(self.train_dict, no_of_samples, resize)\n",
        "\n",
        "    def test(self, resize=True, verbose=True):\n",
        "        correct_count = 0\n",
        "        samples = 0\n",
        "\n",
        "        for person in tqdm(self.test_dict):\n",
        "            for image in self.test_dict[person]:\n",
        "                try:\n",
        "                    test_image = face_recognition.load_image_file(image)\n",
        "                    if resize: test_image = self.img_resize(test_image, self.RESIZE_DIM_INFERENCE)\n",
        "\n",
        "                    face_locations = face_recognition.face_locations(test_image)\n",
        "\n",
        "                    if len(face_locations) > 0:\n",
        "                        samples += 1\n",
        "                        test_image_enc = face_recognition.face_encodings(test_image)[0]\n",
        "                        face_distances = face_recognition.face_distance(self.encodings, test_image_enc)\n",
        "                        min_distance = np.amin(face_distances)\n",
        "                        \n",
        "                        if min_distance < self.cutoff:\n",
        "                            min_distances = np.full_like(face_distances, 1)\n",
        "\n",
        "                            for x in np.where(face_distances < self.cutoff): min_distances[x] = face_distances[x]\n",
        "\n",
        "                            min_index = np.where(min_distances == min_distance)[0][0]\n",
        "\n",
        "                            pred_label = self.labels[min_index]\n",
        "\n",
        "                            if person == pred_label:\n",
        "                                # correct_text = \"which is correct\"\n",
        "                                correct_count += 1\n",
        "                            else:\n",
        "                                correct_text = \"which is incorrect\"\n",
        "                                if verbose: print(f\"found {pred_label} in {image} (distance={min_distance}) {correct_text}\")\n",
        "                        else:\n",
        "                            print(f\"no match found for {image}\")\n",
        "                except:\n",
        "                    print(f\"file load error: {image}\")\n",
        "        accuracy = correct_count/samples * 100\n",
        "        print(f\"{accuracy}% correctly classified\")\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "class MLAlg(Pipeline):\n",
        "    def __init__(self, algorithm, clf, algo_spec, train_res, infer_res, train_dir, test_dir):\n",
        "        super().__init__(train_res, infer_res, train_dir, test_dir)\n",
        "        self.ALGORITHM = algorithm\n",
        "        self.clf = clf\n",
        "        self.algo_spec = algo_spec\n",
        "\n",
        "    def train(self, no_of_samples, resize=True):\n",
        "        self.encodings, self.labels = self.extract_encodings(self.train_dict, no_of_samples, resize)\n",
        "        self.clf.fit(self.encodings, self.labels)\n",
        "\n",
        "    def test(self, resize=True, verbose=True):\n",
        "        correct_count = 0\n",
        "        samples = 0\n",
        "\n",
        "        for person in tqdm(self.test_dict):\n",
        "            for image in self.test_dict[person]:\n",
        "                try:\n",
        "                    test_image = face_recognition.load_image_file(image)\n",
        "                    if resize: test_image = self.img_resize(test_image, self.RESIZE_DIM_INFERENCE)\n",
        "\n",
        "                    face_locations = face_recognition.face_locations(test_image)\n",
        "\n",
        "                    if len(face_locations) > 0:\n",
        "                        samples += 1\n",
        "                        test_image_enc = face_recognition.face_encodings(test_image)[0]\n",
        "\n",
        "                        name = self.clf.predict([test_image_enc])\n",
        "\n",
        "                        if person == name[0]:\n",
        "                            # correct_text = \"which is correct\"\n",
        "                            correct_count += 1\n",
        "                        else:\n",
        "                            correct_text = \"which is incorrect\"\n",
        "                            if verbose: print(f\"found {name[0]} in {image} {correct_text}\")  \n",
        "                except:\n",
        "                    print(f\"file load error: {image}\")\n",
        "                    \n",
        "        accuracy = correct_count/samples * 100\n",
        "        print(f\"{accuracy}% correctly classified\")\n",
        "\n",
        "        return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNlCU9p33vKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset = \"trainset\"\n",
        "testset = \"testset\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mCg2TWeVcyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How to run (train and test) a (sklearn-type) classifier\n",
        "\n",
        "# import the classifier: from sklearn import svm\n",
        "# set up the classifier: clf = svm.SVC(gamma=\"scale\")\n",
        "\n",
        "# start an instance of our pipeline: \n",
        "# instance = MLAlg(`name`, clf, special properties of the classifier (as a dict), train resolution, test resolution, train directory, test directory)\n",
        "\n",
        "# run the pipeline: instance.pipeline(low=2, high=5, verbose=True)\n",
        "# or just train: instance.train(no of samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu01jIeOeSis",
        "colab_type": "text"
      },
      "source": [
        "First train run is slower than successive runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frZD9x2J4Smb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "# SVC\n",
        "clf = svm.SVC(gamma='scale')\n",
        "instance = MLAlg(\"svm\", clf, {\"gamma\":\"scale\"}, 200, 400, trainset, testset)\n",
        "instance.pipeline(low=2, high=5, verbose=True) # Set verbose=False to repress details about misclassifications"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U98b8Hqh4owv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "a44a3551-984a-4b4f-d8cf-c4d0ab257407"
      },
      "source": [
        "# Distance-based\n",
        "cutoff = 0.6 # Distance cutoff (between 0 and 1) (higher is stricter)\n",
        "instance = byDistance(200, 300, trainset, testset, cutoff)\n",
        "instance.pipeline(low=2, high=5, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USING 2 SAMPLE(s) PER LABEL\n",
            "training in progress\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 460/883 [10:28<08:39,  1.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainset/01YALEF_subject1/subject01.rightlight.gif can't be used for training and has been skipped\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 462/883 [10:32<10:32,  1.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainset/01YALEF_subject11/subject11.rightlight.gif can't be used for training and has been skipped\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 847/883 [18:20<00:53,  1.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainset/04BRAZIL_140/140-02.jpg can't be used for training and has been skipped\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 883/883 [19:07<00:00,  1.30s/it]\n",
            "  0%|          | 0/883 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time to encode + train: 1148.126654624939\n",
            "testing in progress\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 30/883 [01:25<47:28,  3.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "found 04BRAZIL_71 in testset/04BRAZIL_2/2-02.jpg (distance=0.2711186808227954) which is incorrect\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 82/883 [04:35<34:58,  2.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "found 04BRAZIL_180 in testset/04BRAZIL_67/67-09.jpg (distance=0.5048822130910635) which is incorrect\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|▉         | 84/883 [04:40<32:06,  2.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "found 04BRAZIL_199 in testset/04BRAZIL_69/69-14.jpg (distance=0.44276057667659535) which is incorrect\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 14%|█▎        | 120/883 [06:26<57:36,  4.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "found 05faces94_gotone in testset/05faces94_9336923/9336923.1.jpg (distance=0.0) which is incorrect\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 161/883 [10:16<1:06:51,  5.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "file load error: testset/05faces94_dakram/.dir3_0.wmd\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|█▉        | 174/883 [11:24<1:01:14,  5.18s/it]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}